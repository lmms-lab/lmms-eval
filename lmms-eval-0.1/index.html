<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="EPlMR3j8io1DcEScvNJuBPxLxCTSnNUjNwF4ZBhRO-I"> <meta name="msvalidate.01" content="1A5205C9A74C20D5BFDEB57516895D92"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Accelerating the Development of Large Multimoal Models with LMMs-Eval | LMMS-Eval </title> <meta name="author" content="LMMs-Lab"> <meta name="description" content="One command evaluation API for fast and thorough evaluation of LMMs, providing multi-faceted insights on model performance with over 40 datasets."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/lmms-eval-blog/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link defer rel="stylesheet" href="/lmms-eval-blog/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/lmms-eval-blog/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/lmms-eval-blog/assets/img/icon.png?4e5cda34cd4389d79ff9a59a5cd11fe8"> <link rel="stylesheet" href="/lmms-eval-blog/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lmms-lab.github.io/lmms-eval-blog/lmms-eval-0.1/"> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/lmms-eval-blog/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/lmms-eval-blog/assets/js/distillpub/template.v2.js"></script> <script src="/lmms-eval-blog/assets/js/distillpub/transforms.v2.js"></script> <script src="/lmms-eval-blog/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Accelerating the Development of Large Multimoal Models with LMMs-Eval",
            "description": "One command evaluation API for fast and thorough evaluation of LMMs, providing multi-faceted insights on model performance with over 40 datasets.",
            "published": "March 07, 2024",
            "authors": [
              
              {
                "author": "Bo Li*<d-footnote>Equal contribution on code maintainance.</d-footnote>",
                "authorURL": "https://brianboli.com/",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Peiyuan Zhang*",
                "authorURL": "https://veiled-texture-20c.notion.site/Peiyuan-Zhang-ab24b48621c9491db767a76df860873a",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Kaichen Zhang*",
                "authorURL": "https://github.com/kcz358",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Fanyi Pu*",
                "authorURL": "https://pufanyi.github.io",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Joshua Adrian Cahyono",
                "authorURL": "https://www.linkedin.com/in/joshua-adrian-cahyono-5230b814b",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Xinrun Du",
                "authorURL": "https://scholar.google.com/citations?user=Bn01uUMAAAAJ&hl=zh-CN",
                "affiliations": [
                  {
                    "name": "BUPT, China",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yuhao Dong",
                "authorURL": "https://scholar.google.com/citations?user=kMui170AAAAJ&hl=zh-CN",
                "affiliations": [
                  {
                    "name": "Tsinghua University, China",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Haotian Liu",
                "authorURL": "https://hliu.cc/",
                "affiliations": [
                  {
                    "name": "University of Wisconsin-Madison, USA",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yuanhan Zhang",
                "authorURL": "https://zhangyuanhan-ai.github.io/",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ge Zhang",
                "authorURL": "https://scholar.google.com/citations?user=qyTrq4kAAAAJ&hl=zh-CN",
                "affiliations": [
                  {
                    "name": "University of Waterloo",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yue Xiang",
                "authorURL": "https://xiangyue9607.github.io/",
                "affiliations": [
                  {
                    "name": "USC, USA",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Chunyuan Li",
                "authorURL": "https://chunyuan.li/",
                "affiliations": [
                  {
                    "name": "TikTok/ByteDance",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Ziwei Liu",
                "authorURL": "https://liuziwei7.github.io/",
                "affiliations": [
                  {
                    "name": "NTU, Singapore",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/lmms-eval-blog/"> LMMS-Eval </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/lmms-eval-blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="https://lmms-lab.github.io/">LMMs-Lab </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Accelerating the Development of Large Multimoal Models with LMMs-Eval</h1> <p>One command evaluation API for fast and thorough evaluation of LMMs, providing multi-faceted insights on model performance with over 40 datasets.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#necessity-of-lmms-eval">Necessity of lmms-eval</a> </div> <div> <a href="#model-results">Model Results</a> </div> <div> <a href="#supported-models-datasets">Supported Models/Datasets</a> </div> <div> <a href="#citations">Citations</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In an era where people pursue AGI (Artificial General Intelligence) with the zeal akin to 1960s moon landing mission. Evaluating the core of AGI, the large-scale language models (LLMs) and multi-modality models (LMMs) with unprecedented capabilities, has become a pivotal challenge. These foundation models are at the heart of AGI’s development, representing critical milestones in our quest to achieve intelligent systems that can understand, learn, and interact across a broad range of human tasks.</p> <p>To surmount this, a broad spectrum of datasets is proposed and used to assess model capabilities across various dimensions, creating a comprehensive capability chart that reveals the true performance of models. However, evaluation of models has become quite hard since there are countless evaluation benchmarks and datasets organized in various ways, scattered across the internet, sleeping in somebody’s Google Drive, Dropbox, and other websites hosted by schools or research labs.</p> <p>In the field of language models, there has been a valuable precedent set by the work of <code class="language-plaintext highlighter-rouge">lm-evaluation-harness</code> <d-cite key="eval-harness"></d-cite>. They offer integrated data and model interfaces, enabling rapid evaluation of language models and serving as the backend support framework for the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="external nofollow noopener" target="_blank">open-llm-leaderboard</a>, and has gradually become the underlying ecosystem of the era of foundation models.</p> <p>However, the evaluation of multi-modality models is still in its infancy, and there is no unified evaluation framework that can be used to evaluate multi-modality models across a wide range of datasets. To address this challenge, we introduce <strong>lmms-eval</strong><d-cite key="lmms_eval2024"></d-cite>, an evaluation framework meticulously crafted for consistent and efficient evaluation of Large Multimoal Models (LMMs).</p> <p>We humbly obsorbed the exquisite and efficient design of <a href="https://github.com/EleutherAI/lm-evaluation-harness" rel="external nofollow noopener" target="_blank">lm-evaluation-harness</a>. Building upon its foundation, we implemented our lmms-eval framework with performance optimizations specifically for LMMs.</p> <h2 id="necessity-of-lmms-eval">Necessity of lmms-eval</h2> <p>We believe our effort could provide an efficient interface for the detailed comparison of publicly available models to discern their strengths and weaknesses. It’s also useful for research institutions and production-oriented companies to accelerate the development of Large Multimoal Models. With the <code class="language-plaintext highlighter-rouge">lmms-eval</code>, we have significantly accelerated the lifecycle of model iteration. Inside the LLaVA team, the utilization of <code class="language-plaintext highlighter-rouge">lmms-eval</code> largely improves the efficiency of the model development cycle, as we are able to evaluate weekly trained hundreds of checkpoints on 20-30 datasets, identifying the strengths and weaknesses, and then make targeted improvements.</p> <hr> <h2 id="important-features">Important Features</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/lmms-eval-blog/assets/img/teaser-480.webp 480w,/lmms-eval-blog/assets/img/teaser-800.webp 800w,/lmms-eval-blog/assets/img/teaser-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/lmms-eval-blog/assets/img/teaser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>For more usage guidance, please visit our <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main" rel="external nofollow noopener" target="_blank">GitHub repo</a>.</p> <h3 id="one-command-evaluation-with-detailed-logs-and-samples">One-command evaluation, with detailed logs and samples.</h3> <p>You can evaluate the models on multiple datasets with a single command. No model/data preparation is needed, just one command line, few minutes, and get the results. Not just a result number, but also the detailed logs and samples, including the model args, input question, model response, and ground truth answer.</p> <h3 id="accelerator-support-and-tasks-grouping">Accelerator support and Tasks grouping.</h3> <p>We support the usage of <code class="language-plaintext highlighter-rouge">accelerate</code> to wrap the model for distributed evaluation, supporting multi-gpu and tensor parallelism. With <strong>Task Grouping</strong>, all instances from all tasks are grouped and evaluated in parallel, which significantly improves the throughput of the evaluation.</p> <p>Below are the total runtime on different datasets using 4 x A100 40G.</p> <table> <thead> <tr> <th style="text-align: left">Dataset (#num)</th> <th style="text-align: left">LLaVA-v1.5-7b</th> <th style="text-align: left">LLaVA-v1.5-13b</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">mme (2374)</td> <td style="text-align: left">2 mins 43 seconds</td> <td style="text-align: left">3 mins 27 seconds</td> </tr> <tr> <td style="text-align: left">gqa (12578)</td> <td style="text-align: left">10 mins 43 seconds</td> <td style="text-align: left">14 mins 23 seconds</td> </tr> <tr> <td style="text-align: left">scienceqa_img (2017)</td> <td style="text-align: left">1 mins 58 seconds</td> <td style="text-align: left">2 mins 52 seconds</td> </tr> <tr> <td style="text-align: left">ai2d (3088)</td> <td style="text-align: left">3 mins 17 seconds</td> <td style="text-align: left">4 mins 12 seconds</td> </tr> <tr> <td style="text-align: left">coco2017_cap_val (5000)</td> <td style="text-align: left">14 mins 13 seconds</td> <td style="text-align: left">19 mins 58 seconds</td> </tr> </tbody> </table> <h3 id="all-in-one-hf-dataset-hubs">All-In-One HF dataset hubs.</h3> <p>We are hosting more than 40 (and increasing) datasets on <a href="https://huggingface.co/lmms-lab" rel="external nofollow noopener" target="_blank">huggingface/lmms-lab</a>, we carefully converted these datasets from original sources and included all variants, versions and splits. Now they can be directly accessed without any burden of data preprocessing. They also serve for the purpose of visualizing the data and grasping the sense of evaluation tasks distribution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/lmms-eval-blog/assets/img/ai2d-480.webp 480w,/lmms-eval-blog/assets/img/ai2d-800.webp 800w,/lmms-eval-blog/assets/img/ai2d-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/lmms-eval-blog/assets/img/ai2d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="detailed-logging-utilites">Detailed Logging Utilites</h3> <p>We provide detailed logging utilities to help you understand the evaluation process and results. The logs include the model args, generation parameters, input question, model response, and ground truth answer. You can also record every details and visualize them inside runs on Weights &amp; Biases.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/lmms-eval-blog/assets/img/wandb_table-480.webp 480w,/lmms-eval-blog/assets/img/wandb_table-800.webp 800w,/lmms-eval-blog/assets/img/wandb_table-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/lmms-eval-blog/assets/img/wandb_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="model-results">Model Results</h2> <p>As demonstrated by the extensive table below, we aim to provide detailed information for readers to understand the datasets included in lmms-eval and some specific details about these datasets (we remain grateful for any corrections readers may have during our evaluation process).</p> <p>We provide a Google Sheet for the detailed results of the LLaVA series models on different datasets. You can access the sheet <a href="https://docs.google.com/spreadsheets/d/1a5ImfdKATDI8T7Cwh6eH-bEsnQFzanFraFUgcS9KHWc/edit?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>. It’s a live sheet, and we are updating it with new results.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/lmms-eval-blog/assets/img/sheet_table-480.webp 480w,/lmms-eval-blog/assets/img/sheet_table-800.webp 800w,/lmms-eval-blog/assets/img/sheet_table-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/lmms-eval-blog/assets/img/sheet_table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We also provide the raw data exported from Weights &amp; Biases for the detailed results of the LLaVA series models on different datasets. You can access the raw data <a href="https://docs.google.com/spreadsheets/d/1AvaEmuG4csSmXaHjgu4ei1KBMmNNW8wflOD_kkTDdv8/edit?usp=sharing" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="supported-modelsdatasets">Supported Models/Datasets</h2> <p>Different models perform best at specific prompt strategies and require models developers skilled knowledge to implement. We prefer not to hastily integrate a model without a thorough understanding to it. Our focus is on more tasks, and we encourage model developers to incorporate our framework into their development process and, when appropriate, integrate their model implementations into our framework through PRs (Pull Requests). Within this strategy, we support a wide range of datasets and selective model series, including:</p> <h3 id="supported-models">Supported models</h3> <ul> <li>GPT4V (API, only generation-based evaluation)</li> <li>LLaVA series (ppl-based, generation-based)</li> <li>Qwen-VL series (ppl-based, generation-based)</li> <li>Fuyu series (ppl-based, generation-based)</li> <li>InstructBLIP series (generation-based)</li> <li>MiniCPM-V series (ppl-based, generation-based)</li> </ul> <h3 id="supported-datasets">Supported datasets</h3> <p>Names inside <code class="language-plaintext highlighter-rouge">()</code> indicate the actual task name referred in the config file.</p> <ul> <li>AI2D <code class="language-plaintext highlighter-rouge">(ai2d)</code> </li> <li>ChartQA <code class="language-plaintext highlighter-rouge">(chartqa)</code> </li> <li>CMMMU <code class="language-plaintext highlighter-rouge">(cmmmu)</code> <ul> <li>CMMMU Validation <code class="language-plaintext highlighter-rouge">(cmmmu_val)</code> </li> <li>CMMMU Test <code class="language-plaintext highlighter-rouge">(cmmmu_test)</code> </li> </ul> </li> <li>COCO Caption <code class="language-plaintext highlighter-rouge">(coco_cap)</code> <ul> <li>COCO 2014 Caption <code class="language-plaintext highlighter-rouge">(coco2014_cap)</code> <ul> <li>COCO 2014 Caption Validation <code class="language-plaintext highlighter-rouge">(coco2014_cap_val)</code> </li> <li>COCO 2014 Caption Test <code class="language-plaintext highlighter-rouge">(coco2014_cap_test)</code> </li> </ul> </li> <li>COCO 2017 Caption <code class="language-plaintext highlighter-rouge">(coco2017_cap)</code> <ul> <li>COCO 2017 Caption MiniVal <code class="language-plaintext highlighter-rouge">(coco2017_cap_val)</code> </li> <li>COCO 2017 Caption MiniTest <code class="language-plaintext highlighter-rouge">(coco2017_cap_test)</code> </li> </ul> </li> </ul> </li> <li>DOCVQA <code class="language-plaintext highlighter-rouge">(docvqa)</code> <ul> <li>DOCVQA Validation <code class="language-plaintext highlighter-rouge">(docvqa_val)</code> </li> <li>DOCVQA Test <code class="language-plaintext highlighter-rouge">(docvqa_test)</code> </li> </ul> </li> <li>Ferret <code class="language-plaintext highlighter-rouge">(ferret)</code> </li> <li>Flickr30K <code class="language-plaintext highlighter-rouge">(flickr30k)</code> <ul> <li>Ferret Test <code class="language-plaintext highlighter-rouge">(ferret_test)</code> </li> </ul> </li> <li>GQA <code class="language-plaintext highlighter-rouge">(gqa)</code> </li> <li>HallusionBenchmark <code class="language-plaintext highlighter-rouge">(hallusion_bench_image)</code> </li> <li>Infographic VQA <code class="language-plaintext highlighter-rouge">(info_vqa)</code> <ul> <li>Infographic VQA Validation <code class="language-plaintext highlighter-rouge">(info_vqa_val)</code> </li> <li>Infographic VQA Test <code class="language-plaintext highlighter-rouge">(info_vqa_test)</code> </li> </ul> </li> <li>LLaVA-Bench <code class="language-plaintext highlighter-rouge">(llava_bench_wild)</code> </li> <li>LLaVA-Bench-COCO <code class="language-plaintext highlighter-rouge">(llava_bench_coco)</code> </li> <li>MathVista <code class="language-plaintext highlighter-rouge">(mathvista)</code> <ul> <li>MathVista Validation <code class="language-plaintext highlighter-rouge">(mathvista_testmini)</code> </li> <li>MathVista Test <code class="language-plaintext highlighter-rouge">(mathvista_test)</code> </li> </ul> </li> <li>MMBench <code class="language-plaintext highlighter-rouge">(mmbench)</code> <ul> <li>MMBench English <code class="language-plaintext highlighter-rouge">(mmbench_en)</code> <ul> <li>MMBench English Dev <code class="language-plaintext highlighter-rouge">(mmbench_en_dev)</code> </li> <li>MMBench English Test <code class="language-plaintext highlighter-rouge">(mmbench_en_test)</code> </li> </ul> </li> <li>MMBench Chinese <code class="language-plaintext highlighter-rouge">(mmbench_cn)</code> <ul> <li>MMBench Chinese Dev <code class="language-plaintext highlighter-rouge">(mmbench_cn_dev)</code> </li> <li>MMBench Chinese Test <code class="language-plaintext highlighter-rouge">(mmbench_cn_test)</code> </li> </ul> </li> </ul> </li> <li>MME <code class="language-plaintext highlighter-rouge">(mme)</code> </li> <li>MMMU <code class="language-plaintext highlighter-rouge">(mmmu)</code> <ul> <li>MMMU Validation <code class="language-plaintext highlighter-rouge">(mmmu_val)</code> </li> <li>MMMU Test <code class="language-plaintext highlighter-rouge">(mmmu_test)</code> </li> </ul> </li> <li>MMVet <code class="language-plaintext highlighter-rouge">(mmvet)</code> </li> <li>Multi-DocVQA <code class="language-plaintext highlighter-rouge">(multidocvqa)</code> <ul> <li>Multi-DocVQA Validation <code class="language-plaintext highlighter-rouge">(multidocvqa_val)</code> </li> <li>Multi-DocVQA Test <code class="language-plaintext highlighter-rouge">(multidocvqa_test)</code> </li> </ul> </li> </ul> <p>… and more.</p> <h2 id="citations">Citations</h2> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc<span class="o">{</span>lmms_eval2024,
    <span class="nv">title</span><span class="o">={</span>LMMs-Eval: Accelerating the Development of Large Multimoal Models<span class="o">}</span>,
    <span class="nv">url</span><span class="o">={</span>https://github.com/EvolvingLMMs-Lab/lmms-eval<span class="o">}</span>,
    <span class="nv">author</span><span class="o">={</span>Bo Li<span class="k">*</span>, Peiyuan Zhang<span class="k">*</span>, Kaicheng Zhang<span class="k">*</span>, Fanyi Pu<span class="k">*</span>, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Yue Xiang, Chunyuan Li and Ziwei Liu<span class="o">}</span>,
    publisher    <span class="o">=</span> <span class="o">{</span>Zenodo<span class="o">}</span>,
    version      <span class="o">=</span> <span class="o">{</span>v0.1.0<span class="o">}</span>,
    <span class="nv">month</span><span class="o">={</span>March<span class="o">}</span>,
    <span class="nv">year</span><span class="o">={</span>2024<span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/lmms-eval-blog/assets/bibliography/2024-03-05-lmms-eval-1.0.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 <a href="https://lmms-lab.github.io/">LMMs-Lab</a>. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: March 08, 2024. </div> </footer> <script src="/lmms-eval-blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>